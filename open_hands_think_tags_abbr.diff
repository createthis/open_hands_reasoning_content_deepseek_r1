diff --git a/openhands/llm/llm.py b/openhands/llm/llm.py
index 787e0431e..5ebcc3e1e 100644
--- a/openhands/llm/llm.py
+++ b/openhands/llm/llm.py
@@ -1,5 +1,6 @@
 import copy
 import os
+import re
 import time
 import warnings
 from functools import partial
@@ -240,6 +241,16 @@ class LLM(RetryMixin, DebugMixin):
                 messages_kwarg if isinstance(messages_kwarg, list) else [messages_kwarg]
             )
 
+            # ---- strip any previous [think] blocks before re-sending ----
+            for _m in messages:
+                if isinstance(_m, dict) and _m.get('role') == 'assistant':
+                    c = _m.get('content')
+                    if isinstance(c, str):
+                        _m['content'] = re.sub(
+                            r'\[think\].*?\[/think\]\s*', '', c, flags=re.S
+                        )
+            # -------------------------------------------------------------
+
             # handle conversion of to non-function calling messages if needed
             original_fncall_messages = copy.deepcopy(messages)
             mock_fncall_tools = None
@@ -307,8 +318,20 @@ class LLM(RetryMixin, DebugMixin):
                     message=r'.*content=.*upload.*',
                     category=DeprecationWarning,
                 )
+
                 resp: ModelResponse = self._completion_unwrapped(*args, **kwargs)
 
+                # ---- add [think] block for UI visibility --------------------
+                try:
+                    _msg = resp['choices'][0]['message']  # dict-like
+                    _rc = _msg.get('reasoning_content')
+                    if _rc:
+                        _visible = _msg.get('content', '') or ''
+                        _msg['content'] = f'[think]{_rc.strip()}[/think]\n{_visible}'
+                except Exception as e:
+                    logger.debug(f'reasoning merge skipped: {e}')
+                # -------------------------------------------------------------
+
             # Calculate and record latency
             latency = time.time() - start_time
             response_id = resp.get('id', 'unknown')
diff --git a/openhands/utils/conversation_summary.py b/openhands/utils/conversation_summary.py
index c2fc0b75d..f0930678f 100644
--- a/openhands/utils/conversation_summary.py
+++ b/openhands/utils/conversation_summary.py
@@ -1,5 +1,6 @@
 """Utility functions for generating conversation summaries."""
 
+import re
 from typing import Optional
 
 from openhands.core.config import LLMConfig
@@ -12,6 +13,11 @@ from openhands.storage.data_models.settings import Settings
 from openhands.storage.files import FileStore
 
 
+def strip_thinking_block(text: str) -> str:
+    """Remove [think]...[/think] block from a string."""
+    return re.sub(r'\[think\].*?\[/think\]\s*', '', text, flags=re.DOTALL)
+
+
 async def generate_conversation_title(
     message: str, llm_config: LLMConfig, max_length: int = 50
 ) -> Optional[str]:
@@ -28,6 +34,9 @@ async def generate_conversation_title(
     if not message or message.strip() == '':
         return None
 
+    # Strip [think]...[/think] block before anything else
+    message = strip_thinking_block(message)
+
     # Truncate very long messages to avoid excessive token usage
     if len(message) > 1000:
         truncated_message = message[:1000] + '...(truncated)'
@@ -50,7 +59,7 @@ async def generate_conversation_title(
         ]
 
         response = llm.completion(messages=messages)
-        title = response.choices[0].message.content.strip()
+        title = strip_thinking_block(response.choices[0].message.content.strip())
 
         # Ensure the title isn't too long
         if len(title) > max_length:
